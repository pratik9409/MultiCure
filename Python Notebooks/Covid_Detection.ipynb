{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TwLQgTrI1H5",
    "outputId": "c6c59a6d-de24-4905-dde2-a5b799c3526c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ouB_ppQ5I44Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import  Flatten, Dense, Dropout\n",
    "from keras.applications import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, AveragePooling2D, GlobalAveragePooling2D\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SmBiBnfJA-p",
    "outputId": "2e97f908-8e74-41c4-9278-2d965a4fedaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/covid_data\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/\"My Drive\"/\"covid_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0kfuTdfJWYu",
    "outputId": "fd3395cc-eeb6-4375-987f-54d5d21ec4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /content/gdrive/MyDrive/covid_data. Or use the environment method.\n",
      "unzip:  cannot find or open *.zip, *.zip.zip or *.zip.ZIP.\n",
      "\n",
      "No zipfiles found.\n"
     ]
    }
   ],
   "source": [
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/covid_data\"\n",
    "!kaggle datasets download -d plameneduardo/sarscov2-ctscan-dataset\n",
    "!unzip \\*.zip  && rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6cws_FydJlmd"
   },
   "outputs": [],
   "source": [
    "disease_types=['COVID', 'non-COVID']\n",
    "data_dir = '/content/gdrive/MyDrive/covid_data'\n",
    "train_dir = os.path.join(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LXO-clOfLPJ3"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for defects_id, sp in enumerate(disease_types):\n",
    "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
    "        train_data.append(['{}/{}'.format(sp, file), defects_id, sp])      \n",
    "train = pd.DataFrame(train_data, columns=['File', 'DiseaseID','Disease Type'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i7OE7DoWLzBn"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 150\n",
    "def read_image(filepath):\n",
    "    return cv2.imread(os.path.join(data_dir, filepath)) \n",
    "def resize_image(image, image_size):\n",
    "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbxvYgjWLUb2",
    "outputId": "dd6e3a59-9ede-4eaa-a1f0-a09e835b990f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2481it [05:24,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2481, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "for i, file in tqdm(enumerate(train['File'].values)):\n",
    "    image = read_image(file)\n",
    "    if image is not None:\n",
    "        X_train[i] = resize_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "X_Train = X_train / 255.\n",
    "print(X_Train.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aHLjzDd6LtPE"
   },
   "outputs": [],
   "source": [
    "Y_train = train['DiseaseID'].values\n",
    "Y_train = to_categorical(Y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "68k6ujQwL4Uy"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_Train, Y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGJcUDQeL6zL",
    "outputId": "8c73788b-0257-4608-a73b-b7e1bc789a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = VGG16(weights = 'imagenet', include_top = False,input_shape=(150,150,3))\n",
    "x = vgg16_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Yz8eIPX7L_S_"
   },
   "outputs": [],
   "source": [
    "predictions = Dense(2, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XJpp1dHTL_s7"
   },
   "outputs": [],
   "source": [
    "model = Model(vgg16_model.input,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Cx1QKo9ML_x2"
   },
   "outputs": [],
   "source": [
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0J-jd1HL_19",
    "outputId": "d8c45e6b-ff1f-4c23-804c-5cbbde18eaec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = Adam(lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4EDYqSjL_5G",
    "outputId": "6922b732-bcf3-4a84-eb79-12e6fdc86e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Resnet50_Architecture\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,849,602\n",
      "Trainable params: 133,378\n",
      "Non-trainable params: 14,716,224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model._name = \"Resnet50_Architecture\"\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nc0BYpn9L_8K"
   },
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator(rotation_range=40, \n",
    "                        width_shift_range=0.2, \n",
    "                        height_shift_range=0.2, \n",
    "                        zoom_range=0.2, \n",
    "                        horizontal_flip=True, \n",
    "                        vertical_flip=True,\n",
    "                        shear_range=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ZkHpV1bOL_-3"
   },
   "outputs": [],
   "source": [
    "train_generator = training_datagen.flow(X_train, Y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XF0C24rtMABc"
   },
   "outputs": [],
   "source": [
    "training_datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "u1P8sJcAMAEG"
   },
   "outputs": [],
   "source": [
    "filepath=\"vgg16_model.h5\"\n",
    "checkpoint_1 = callbacks.ModelCheckpoint(filepath, monitor= 'val_loss',save_best_only=True, mode='min',verbose=1)\n",
    "checkpoint_2 = callbacks.ModelCheckpoint(filepath, monitor= 'val_accuracy',save_best_only=True, mode='max',verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint_1, checkpoint_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTWmtXvDMAI1",
    "outputId": "b9fd8781-c931-4bb5-c6a8-68f168202348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.9141 - accuracy: 0.5775\n",
      "Epoch 1: val_loss improved from inf to 0.68370, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69880, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 20s 522ms/step - loss: 0.9141 - accuracy: 0.5775 - val_loss: 0.6837 - val_accuracy: 0.6988\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.6279\n",
      "Epoch 2: val_loss improved from 0.68370 to 0.65141, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.69880\n",
      "32/32 [==============================] - 13s 391ms/step - loss: 0.8269 - accuracy: 0.6279 - val_loss: 0.6514 - val_accuracy: 0.6506\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7304 - accuracy: 0.6868\n",
      "Epoch 3: val_loss improved from 0.65141 to 0.62959, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.69880 to 0.72289, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 406ms/step - loss: 0.7304 - accuracy: 0.6868 - val_loss: 0.6296 - val_accuracy: 0.7229\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7142 - accuracy: 0.6966\n",
      "Epoch 4: val_loss improved from 0.62959 to 0.61184, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.72289\n",
      "32/32 [==============================] - 13s 394ms/step - loss: 0.7142 - accuracy: 0.6966 - val_loss: 0.6118 - val_accuracy: 0.7149\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7072 - accuracy: 0.6877\n",
      "Epoch 5: val_loss improved from 0.61184 to 0.59642, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.72289\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 0.7072 - accuracy: 0.6877 - val_loss: 0.5964 - val_accuracy: 0.7149\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.7049 - accuracy: 0.6868\n",
      "Epoch 6: val_loss improved from 0.59642 to 0.58248, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.72289\n",
      "32/32 [==============================] - 14s 432ms/step - loss: 0.7049 - accuracy: 0.6868 - val_loss: 0.5825 - val_accuracy: 0.6988\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6754 - accuracy: 0.7020\n",
      "Epoch 7: val_loss improved from 0.58248 to 0.56306, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 7: val_accuracy improved from 0.72289 to 0.72691, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 400ms/step - loss: 0.6754 - accuracy: 0.7020 - val_loss: 0.5631 - val_accuracy: 0.7269\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6515 - accuracy: 0.7020\n",
      "Epoch 8: val_loss improved from 0.56306 to 0.54116, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.72691 to 0.74699, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 399ms/step - loss: 0.6515 - accuracy: 0.7020 - val_loss: 0.5412 - val_accuracy: 0.7470\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.7186\n",
      "Epoch 9: val_loss improved from 0.54116 to 0.52928, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.74699 to 0.75904, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 409ms/step - loss: 0.6479 - accuracy: 0.7186 - val_loss: 0.5293 - val_accuracy: 0.7590\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.7059\n",
      "Epoch 10: val_loss improved from 0.52928 to 0.51461, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.75904 to 0.76305, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 400ms/step - loss: 0.6373 - accuracy: 0.7059 - val_loss: 0.5146 - val_accuracy: 0.7631\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.7010\n",
      "Epoch 11: val_loss improved from 0.51461 to 0.49881, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.76305\n",
      "32/32 [==============================] - 13s 390ms/step - loss: 0.6521 - accuracy: 0.7010 - val_loss: 0.4988 - val_accuracy: 0.7631\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6171 - accuracy: 0.7152\n",
      "Epoch 12: val_loss improved from 0.49881 to 0.48958, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.76305\n",
      "32/32 [==============================] - 13s 403ms/step - loss: 0.6171 - accuracy: 0.7152 - val_loss: 0.4896 - val_accuracy: 0.7550\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.7250\n",
      "Epoch 13: val_loss improved from 0.48958 to 0.48020, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.76305 to 0.77912, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 407ms/step - loss: 0.6445 - accuracy: 0.7250 - val_loss: 0.4802 - val_accuracy: 0.7791\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6201 - accuracy: 0.7392\n",
      "Epoch 14: val_loss improved from 0.48020 to 0.46885, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.77912 to 0.79116, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 15s 468ms/step - loss: 0.6201 - accuracy: 0.7392 - val_loss: 0.4688 - val_accuracy: 0.7912\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.7422\n",
      "Epoch 15: val_loss improved from 0.46885 to 0.46334, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.79116\n",
      "32/32 [==============================] - 13s 414ms/step - loss: 0.5938 - accuracy: 0.7422 - val_loss: 0.4633 - val_accuracy: 0.7751\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6087 - accuracy: 0.7270\n",
      "Epoch 16: val_loss improved from 0.46334 to 0.45557, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.79116\n",
      "32/32 [==============================] - 13s 398ms/step - loss: 0.6087 - accuracy: 0.7270 - val_loss: 0.4556 - val_accuracy: 0.7751\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7334\n",
      "Epoch 17: val_loss improved from 0.45557 to 0.45225, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.79116 to 0.79518, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 14s 432ms/step - loss: 0.6006 - accuracy: 0.7334 - val_loss: 0.4523 - val_accuracy: 0.7952\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.7328\n",
      "Epoch 18: val_loss improved from 0.45225 to 0.44505, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.79518 to 0.80723, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 419ms/step - loss: 0.5901 - accuracy: 0.7328 - val_loss: 0.4450 - val_accuracy: 0.8072\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5875 - accuracy: 0.7397\n",
      "Epoch 19: val_loss did not improve from 0.44505\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.80723\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.5875 - accuracy: 0.7397 - val_loss: 0.4455 - val_accuracy: 0.7952\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.7348\n",
      "Epoch 20: val_loss improved from 0.44505 to 0.43751, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 20: val_accuracy improved from 0.80723 to 0.81124, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 0.5874 - accuracy: 0.7348 - val_loss: 0.4375 - val_accuracy: 0.8112\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.7368\n",
      "Epoch 21: val_loss improved from 0.43751 to 0.43545, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 389ms/step - loss: 0.5823 - accuracy: 0.7368 - val_loss: 0.4355 - val_accuracy: 0.7952\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7539\n",
      "Epoch 22: val_loss improved from 0.43545 to 0.43105, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 0.5669 - accuracy: 0.7539 - val_loss: 0.4310 - val_accuracy: 0.7992\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.7412\n",
      "Epoch 23: val_loss improved from 0.43105 to 0.43003, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 0.5836 - accuracy: 0.7412 - val_loss: 0.4300 - val_accuracy: 0.7992\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6014 - accuracy: 0.7201\n",
      "Epoch 24: val_loss improved from 0.43003 to 0.42766, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 387ms/step - loss: 0.6014 - accuracy: 0.7201 - val_loss: 0.4277 - val_accuracy: 0.7952\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7426\n",
      "Epoch 25: val_loss improved from 0.42766 to 0.42481, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 388ms/step - loss: 0.5621 - accuracy: 0.7426 - val_loss: 0.4248 - val_accuracy: 0.8032\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7319\n",
      "Epoch 26: val_loss did not improve from 0.42481\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.5803 - accuracy: 0.7319 - val_loss: 0.4252 - val_accuracy: 0.8032\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.7637\n",
      "Epoch 27: val_loss improved from 0.42481 to 0.42458, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 409ms/step - loss: 0.5513 - accuracy: 0.7637 - val_loss: 0.4246 - val_accuracy: 0.8032\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.7392\n",
      "Epoch 28: val_loss did not improve from 0.42458\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.5717 - accuracy: 0.7392 - val_loss: 0.4280 - val_accuracy: 0.7952\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7510\n",
      "Epoch 29: val_loss improved from 0.42458 to 0.42293, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 383ms/step - loss: 0.5541 - accuracy: 0.7510 - val_loss: 0.4229 - val_accuracy: 0.7912\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.7402\n",
      "Epoch 30: val_loss improved from 0.42293 to 0.42240, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 432ms/step - loss: 0.5485 - accuracy: 0.7402 - val_loss: 0.4224 - val_accuracy: 0.7992\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.7422\n",
      "Epoch 31: val_loss improved from 0.42240 to 0.42045, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 0.5501 - accuracy: 0.7422 - val_loss: 0.4204 - val_accuracy: 0.7912\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5397 - accuracy: 0.7554\n",
      "Epoch 32: val_loss did not improve from 0.42045\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.5397 - accuracy: 0.7554 - val_loss: 0.4212 - val_accuracy: 0.7912\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.7397\n",
      "Epoch 33: val_loss did not improve from 0.42045\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.5544 - accuracy: 0.7397 - val_loss: 0.4218 - val_accuracy: 0.7871\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.7485\n",
      "Epoch 34: val_loss did not improve from 0.42045\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.5434 - accuracy: 0.7485 - val_loss: 0.4248 - val_accuracy: 0.7871\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.7647\n",
      "Epoch 35: val_loss did not improve from 0.42045\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.5173 - accuracy: 0.7647 - val_loss: 0.4220 - val_accuracy: 0.7912\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7446\n",
      "Epoch 36: val_loss improved from 0.42045 to 0.41948, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 390ms/step - loss: 0.5461 - accuracy: 0.7446 - val_loss: 0.4195 - val_accuracy: 0.7992\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5424 - accuracy: 0.7485\n",
      "Epoch 37: val_loss did not improve from 0.41948\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.5424 - accuracy: 0.7485 - val_loss: 0.4212 - val_accuracy: 0.7952\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5188 - accuracy: 0.7568\n",
      "Epoch 38: val_loss did not improve from 0.41948\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.5188 - accuracy: 0.7568 - val_loss: 0.4216 - val_accuracy: 0.7831\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.7559\n",
      "Epoch 39: val_loss improved from 0.41948 to 0.41584, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 422ms/step - loss: 0.5291 - accuracy: 0.7559 - val_loss: 0.4158 - val_accuracy: 0.7912\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7466\n",
      "Epoch 40: val_loss improved from 0.41584 to 0.41554, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 389ms/step - loss: 0.5466 - accuracy: 0.7466 - val_loss: 0.4155 - val_accuracy: 0.7952\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.7578\n",
      "Epoch 41: val_loss did not improve from 0.41554\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.5306 - accuracy: 0.7578 - val_loss: 0.4179 - val_accuracy: 0.7952\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.7544\n",
      "Epoch 42: val_loss did not improve from 0.41554\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.5212 - accuracy: 0.7544 - val_loss: 0.4163 - val_accuracy: 0.7952\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5180 - accuracy: 0.7613\n",
      "Epoch 43: val_loss improved from 0.41554 to 0.41489, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 390ms/step - loss: 0.5180 - accuracy: 0.7613 - val_loss: 0.4149 - val_accuracy: 0.7952\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.7475\n",
      "Epoch 44: val_loss improved from 0.41489 to 0.41427, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 429ms/step - loss: 0.5292 - accuracy: 0.7475 - val_loss: 0.4143 - val_accuracy: 0.7952\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.7618\n",
      "Epoch 45: val_loss improved from 0.41427 to 0.41343, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 400ms/step - loss: 0.5264 - accuracy: 0.7618 - val_loss: 0.4134 - val_accuracy: 0.7992\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.7417\n",
      "Epoch 46: val_loss did not improve from 0.41343\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 407ms/step - loss: 0.5323 - accuracy: 0.7417 - val_loss: 0.4148 - val_accuracy: 0.8072\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.7554\n",
      "Epoch 47: val_loss improved from 0.41343 to 0.41296, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 392ms/step - loss: 0.5210 - accuracy: 0.7554 - val_loss: 0.4130 - val_accuracy: 0.7992\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5257 - accuracy: 0.7402\n",
      "Epoch 48: val_loss did not improve from 0.41296\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.5257 - accuracy: 0.7402 - val_loss: 0.4138 - val_accuracy: 0.8032\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.7471\n",
      "Epoch 49: val_loss improved from 0.41296 to 0.41138, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 389ms/step - loss: 0.5293 - accuracy: 0.7471 - val_loss: 0.4114 - val_accuracy: 0.8072\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5302 - accuracy: 0.7456\n",
      "Epoch 50: val_loss did not improve from 0.41138\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.5302 - accuracy: 0.7456 - val_loss: 0.4141 - val_accuracy: 0.7992\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.7534\n",
      "Epoch 51: val_loss did not improve from 0.41138\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.5169 - accuracy: 0.7534 - val_loss: 0.4133 - val_accuracy: 0.7992\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.7387\n",
      "Epoch 52: val_loss improved from 0.41138 to 0.41118, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 395ms/step - loss: 0.5248 - accuracy: 0.7387 - val_loss: 0.4112 - val_accuracy: 0.7992\n",
      "Epoch 53/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4943 - accuracy: 0.7672\n",
      "Epoch 53: val_loss did not improve from 0.41118\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4943 - accuracy: 0.7672 - val_loss: 0.4116 - val_accuracy: 0.7952\n",
      "Epoch 54/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.7451\n",
      "Epoch 54: val_loss did not improve from 0.41118\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.5154 - accuracy: 0.7451 - val_loss: 0.4127 - val_accuracy: 0.7912\n",
      "Epoch 55/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7529\n",
      "Epoch 55: val_loss improved from 0.41118 to 0.40993, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 438ms/step - loss: 0.5062 - accuracy: 0.7529 - val_loss: 0.4099 - val_accuracy: 0.7992\n",
      "Epoch 56/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7618\n",
      "Epoch 56: val_loss did not improve from 0.40993\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.5025 - accuracy: 0.7618 - val_loss: 0.4103 - val_accuracy: 0.7871\n",
      "Epoch 57/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4961 - accuracy: 0.7686\n",
      "Epoch 57: val_loss improved from 0.40993 to 0.40910, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 393ms/step - loss: 0.4961 - accuracy: 0.7686 - val_loss: 0.4091 - val_accuracy: 0.7952\n",
      "Epoch 58/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.7515\n",
      "Epoch 58: val_loss improved from 0.40910 to 0.40905, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 401ms/step - loss: 0.5129 - accuracy: 0.7515 - val_loss: 0.4090 - val_accuracy: 0.7912\n",
      "Epoch 59/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4974 - accuracy: 0.7613\n",
      "Epoch 59: val_loss improved from 0.40905 to 0.40823, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 393ms/step - loss: 0.4974 - accuracy: 0.7613 - val_loss: 0.4082 - val_accuracy: 0.7912\n",
      "Epoch 60/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.7569\n",
      "Epoch 60: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.5073 - accuracy: 0.7569 - val_loss: 0.4122 - val_accuracy: 0.7871\n",
      "Epoch 61/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.7534\n",
      "Epoch 61: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.5067 - accuracy: 0.7534 - val_loss: 0.4125 - val_accuracy: 0.7871\n",
      "Epoch 62/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.7710\n",
      "Epoch 62: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4893 - accuracy: 0.7710 - val_loss: 0.4132 - val_accuracy: 0.7912\n",
      "Epoch 63/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.7613\n",
      "Epoch 63: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4969 - accuracy: 0.7613 - val_loss: 0.4108 - val_accuracy: 0.7952\n",
      "Epoch 64/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4941 - accuracy: 0.7760\n",
      "Epoch 64: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 403ms/step - loss: 0.4941 - accuracy: 0.7760 - val_loss: 0.4139 - val_accuracy: 0.7952\n",
      "Epoch 65/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.7637\n",
      "Epoch 65: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4939 - accuracy: 0.7637 - val_loss: 0.4151 - val_accuracy: 0.8032\n",
      "Epoch 66/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7564\n",
      "Epoch 66: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4954 - accuracy: 0.7564 - val_loss: 0.4154 - val_accuracy: 0.7992\n",
      "Epoch 67/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.7691\n",
      "Epoch 67: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4835 - accuracy: 0.7691 - val_loss: 0.4122 - val_accuracy: 0.7912\n",
      "Epoch 68/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.7471\n",
      "Epoch 68: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 375ms/step - loss: 0.5101 - accuracy: 0.7471 - val_loss: 0.4130 - val_accuracy: 0.7952\n",
      "Epoch 69/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.7770\n",
      "Epoch 69: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4630 - accuracy: 0.7770 - val_loss: 0.4141 - val_accuracy: 0.7912\n",
      "Epoch 70/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4963 - accuracy: 0.7623\n",
      "Epoch 70: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 361ms/step - loss: 0.4963 - accuracy: 0.7623 - val_loss: 0.4150 - val_accuracy: 0.7871\n",
      "Epoch 71/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4857 - accuracy: 0.7656\n",
      "Epoch 71: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4857 - accuracy: 0.7656 - val_loss: 0.4134 - val_accuracy: 0.7912\n",
      "Epoch 72/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.7627\n",
      "Epoch 72: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4875 - accuracy: 0.7627 - val_loss: 0.4118 - val_accuracy: 0.7831\n",
      "Epoch 73/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.7642\n",
      "Epoch 73: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.5013 - accuracy: 0.7642 - val_loss: 0.4127 - val_accuracy: 0.7831\n",
      "Epoch 74/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.7627\n",
      "Epoch 74: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4970 - accuracy: 0.7627 - val_loss: 0.4166 - val_accuracy: 0.7871\n",
      "Epoch 75/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.7765\n",
      "Epoch 75: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4788 - accuracy: 0.7765 - val_loss: 0.4143 - val_accuracy: 0.7912\n",
      "Epoch 76/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.7573\n",
      "Epoch 76: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.5071 - accuracy: 0.7573 - val_loss: 0.4127 - val_accuracy: 0.7912\n",
      "Epoch 77/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.7529\n",
      "Epoch 77: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 359ms/step - loss: 0.4957 - accuracy: 0.7529 - val_loss: 0.4095 - val_accuracy: 0.7912\n",
      "Epoch 78/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.7828\n",
      "Epoch 78: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4835 - accuracy: 0.7828 - val_loss: 0.4125 - val_accuracy: 0.7952\n",
      "Epoch 79/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.7681\n",
      "Epoch 79: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4859 - accuracy: 0.7681 - val_loss: 0.4109 - val_accuracy: 0.7831\n",
      "Epoch 80/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4919 - accuracy: 0.7588\n",
      "Epoch 80: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 382ms/step - loss: 0.4919 - accuracy: 0.7588 - val_loss: 0.4118 - val_accuracy: 0.7952\n",
      "Epoch 81/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.7696\n",
      "Epoch 81: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4991 - accuracy: 0.7696 - val_loss: 0.4133 - val_accuracy: 0.7952\n",
      "Epoch 82/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.7725\n",
      "Epoch 82: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4866 - accuracy: 0.7725 - val_loss: 0.4139 - val_accuracy: 0.7992\n",
      "Epoch 83/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4888 - accuracy: 0.7612\n",
      "Epoch 83: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4888 - accuracy: 0.7612 - val_loss: 0.4137 - val_accuracy: 0.7831\n",
      "Epoch 84/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4749 - accuracy: 0.7705\n",
      "Epoch 84: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4749 - accuracy: 0.7705 - val_loss: 0.4128 - val_accuracy: 0.7751\n",
      "Epoch 85/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.7569\n",
      "Epoch 85: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4977 - accuracy: 0.7569 - val_loss: 0.4099 - val_accuracy: 0.7831\n",
      "Epoch 86/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.7598\n",
      "Epoch 86: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4850 - accuracy: 0.7598 - val_loss: 0.4140 - val_accuracy: 0.7831\n",
      "Epoch 87/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4881 - accuracy: 0.7711\n",
      "Epoch 87: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4881 - accuracy: 0.7711 - val_loss: 0.4169 - val_accuracy: 0.7711\n",
      "Epoch 88/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.7564\n",
      "Epoch 88: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.5041 - accuracy: 0.7564 - val_loss: 0.4163 - val_accuracy: 0.7871\n",
      "Epoch 89/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7657\n",
      "Epoch 89: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4902 - accuracy: 0.7657 - val_loss: 0.4132 - val_accuracy: 0.7831\n",
      "Epoch 90/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.7603\n",
      "Epoch 90: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 401ms/step - loss: 0.4912 - accuracy: 0.7603 - val_loss: 0.4126 - val_accuracy: 0.7791\n",
      "Epoch 91/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4874 - accuracy: 0.7539\n",
      "Epoch 91: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4874 - accuracy: 0.7539 - val_loss: 0.4128 - val_accuracy: 0.7871\n",
      "Epoch 92/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.7887\n",
      "Epoch 92: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4658 - accuracy: 0.7887 - val_loss: 0.4114 - val_accuracy: 0.7831\n",
      "Epoch 93/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4872 - accuracy: 0.7681\n",
      "Epoch 93: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.4872 - accuracy: 0.7681 - val_loss: 0.4143 - val_accuracy: 0.7952\n",
      "Epoch 94/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.7637\n",
      "Epoch 94: val_loss did not improve from 0.40823\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4803 - accuracy: 0.7637 - val_loss: 0.4125 - val_accuracy: 0.7831\n",
      "Epoch 95/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.7716\n",
      "Epoch 95: val_loss improved from 0.40823 to 0.40820, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 392ms/step - loss: 0.4747 - accuracy: 0.7716 - val_loss: 0.4082 - val_accuracy: 0.7831\n",
      "Epoch 96/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4981 - accuracy: 0.7564\n",
      "Epoch 96: val_loss improved from 0.40820 to 0.40659, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 389ms/step - loss: 0.4981 - accuracy: 0.7564 - val_loss: 0.4066 - val_accuracy: 0.7912\n",
      "Epoch 97/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.7754\n",
      "Epoch 97: val_loss did not improve from 0.40659\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 374ms/step - loss: 0.4762 - accuracy: 0.7754 - val_loss: 0.4084 - val_accuracy: 0.7831\n",
      "Epoch 98/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4882 - accuracy: 0.7681\n",
      "Epoch 98: val_loss did not improve from 0.40659\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4882 - accuracy: 0.7681 - val_loss: 0.4073 - val_accuracy: 0.7871\n",
      "Epoch 99/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.7672\n",
      "Epoch 99: val_loss improved from 0.40659 to 0.40576, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 391ms/step - loss: 0.4755 - accuracy: 0.7672 - val_loss: 0.4058 - val_accuracy: 0.7952\n",
      "Epoch 100/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.7603\n",
      "Epoch 100: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.4954 - accuracy: 0.7603 - val_loss: 0.4093 - val_accuracy: 0.7831\n",
      "Epoch 101/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.7642\n",
      "Epoch 101: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 101: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 361ms/step - loss: 0.4714 - accuracy: 0.7642 - val_loss: 0.4093 - val_accuracy: 0.7791\n",
      "Epoch 102/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4805 - accuracy: 0.7637\n",
      "Epoch 102: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 102: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4805 - accuracy: 0.7637 - val_loss: 0.4073 - val_accuracy: 0.7912\n",
      "Epoch 103/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.7662\n",
      "Epoch 103: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 103: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4781 - accuracy: 0.7662 - val_loss: 0.4105 - val_accuracy: 0.7791\n",
      "Epoch 104/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4699 - accuracy: 0.7735\n",
      "Epoch 104: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 104: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4699 - accuracy: 0.7735 - val_loss: 0.4112 - val_accuracy: 0.7711\n",
      "Epoch 105/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4724 - accuracy: 0.7740\n",
      "Epoch 105: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 105: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4724 - accuracy: 0.7740 - val_loss: 0.4097 - val_accuracy: 0.7751\n",
      "Epoch 106/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.7642\n",
      "Epoch 106: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 106: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 401ms/step - loss: 0.4905 - accuracy: 0.7642 - val_loss: 0.4097 - val_accuracy: 0.7791\n",
      "Epoch 107/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4769 - accuracy: 0.7789\n",
      "Epoch 107: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 107: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.4769 - accuracy: 0.7789 - val_loss: 0.4096 - val_accuracy: 0.7871\n",
      "Epoch 108/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7946\n",
      "Epoch 108: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 108: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4596 - accuracy: 0.7946 - val_loss: 0.4085 - val_accuracy: 0.7912\n",
      "Epoch 109/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4733 - accuracy: 0.7750\n",
      "Epoch 109: val_loss did not improve from 0.40576\n",
      "\n",
      "Epoch 109: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4733 - accuracy: 0.7750 - val_loss: 0.4091 - val_accuracy: 0.7831\n",
      "Epoch 110/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7804\n",
      "Epoch 110: val_loss improved from 0.40576 to 0.40566, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 110: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 391ms/step - loss: 0.4686 - accuracy: 0.7804 - val_loss: 0.4057 - val_accuracy: 0.7912\n",
      "Epoch 111/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4712 - accuracy: 0.7789\n",
      "Epoch 111: val_loss did not improve from 0.40566\n",
      "\n",
      "Epoch 111: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.4712 - accuracy: 0.7789 - val_loss: 0.4066 - val_accuracy: 0.7912\n",
      "Epoch 112/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7760\n",
      "Epoch 112: val_loss improved from 0.40566 to 0.40389, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 112: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 399ms/step - loss: 0.4686 - accuracy: 0.7760 - val_loss: 0.4039 - val_accuracy: 0.7952\n",
      "Epoch 113/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.7676\n",
      "Epoch 113: val_loss improved from 0.40389 to 0.40274, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 113: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 405ms/step - loss: 0.4747 - accuracy: 0.7676 - val_loss: 0.4027 - val_accuracy: 0.7912\n",
      "Epoch 114/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.7770\n",
      "Epoch 114: val_loss did not improve from 0.40274\n",
      "\n",
      "Epoch 114: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 408ms/step - loss: 0.4662 - accuracy: 0.7770 - val_loss: 0.4029 - val_accuracy: 0.7871\n",
      "Epoch 115/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7666\n",
      "Epoch 115: val_loss did not improve from 0.40274\n",
      "\n",
      "Epoch 115: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.4791 - accuracy: 0.7666 - val_loss: 0.4056 - val_accuracy: 0.7751\n",
      "Epoch 116/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4841 - accuracy: 0.7564\n",
      "Epoch 116: val_loss did not improve from 0.40274\n",
      "\n",
      "Epoch 116: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4841 - accuracy: 0.7564 - val_loss: 0.4066 - val_accuracy: 0.7831\n",
      "Epoch 117/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.7848\n",
      "Epoch 117: val_loss did not improve from 0.40274\n",
      "\n",
      "Epoch 117: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4673 - accuracy: 0.7848 - val_loss: 0.4047 - val_accuracy: 0.7791\n",
      "Epoch 118/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.7588\n",
      "Epoch 118: val_loss did not improve from 0.40274\n",
      "\n",
      "Epoch 118: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 0.4775 - accuracy: 0.7588 - val_loss: 0.4054 - val_accuracy: 0.7751\n",
      "Epoch 119/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.7745\n",
      "Epoch 119: val_loss improved from 0.40274 to 0.40249, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 119: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 390ms/step - loss: 0.4630 - accuracy: 0.7745 - val_loss: 0.4025 - val_accuracy: 0.7791\n",
      "Epoch 120/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4678 - accuracy: 0.7838\n",
      "Epoch 120: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 120: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.4678 - accuracy: 0.7838 - val_loss: 0.4057 - val_accuracy: 0.7751\n",
      "Epoch 121/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.7858\n",
      "Epoch 121: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 121: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4529 - accuracy: 0.7858 - val_loss: 0.4066 - val_accuracy: 0.7791\n",
      "Epoch 122/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4557 - accuracy: 0.7775\n",
      "Epoch 122: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 122: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 0.4557 - accuracy: 0.7775 - val_loss: 0.4071 - val_accuracy: 0.7671\n",
      "Epoch 123/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4770 - accuracy: 0.7725\n",
      "Epoch 123: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 123: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4770 - accuracy: 0.7725 - val_loss: 0.4036 - val_accuracy: 0.7711\n",
      "Epoch 124/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7623\n",
      "Epoch 124: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 124: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 400ms/step - loss: 0.4741 - accuracy: 0.7623 - val_loss: 0.4065 - val_accuracy: 0.7831\n",
      "Epoch 125/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4802 - accuracy: 0.7691\n",
      "Epoch 125: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 125: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4802 - accuracy: 0.7691 - val_loss: 0.4058 - val_accuracy: 0.7952\n",
      "Epoch 126/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4570 - accuracy: 0.7907\n",
      "Epoch 126: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 126: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4570 - accuracy: 0.7907 - val_loss: 0.4067 - val_accuracy: 0.7871\n",
      "Epoch 127/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.7765\n",
      "Epoch 127: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 127: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 0.4609 - accuracy: 0.7765 - val_loss: 0.4084 - val_accuracy: 0.7711\n",
      "Epoch 128/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.7779\n",
      "Epoch 128: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 128: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4667 - accuracy: 0.7779 - val_loss: 0.4069 - val_accuracy: 0.7671\n",
      "Epoch 129/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.7711\n",
      "Epoch 129: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 129: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.4673 - accuracy: 0.7711 - val_loss: 0.4047 - val_accuracy: 0.7791\n",
      "Epoch 130/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.7848\n",
      "Epoch 130: val_loss did not improve from 0.40249\n",
      "\n",
      "Epoch 130: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4494 - accuracy: 0.7848 - val_loss: 0.4043 - val_accuracy: 0.7791\n",
      "Epoch 131/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.7760\n",
      "Epoch 131: val_loss improved from 0.40249 to 0.40003, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 131: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 410ms/step - loss: 0.4671 - accuracy: 0.7760 - val_loss: 0.4000 - val_accuracy: 0.7711\n",
      "Epoch 132/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.7750\n",
      "Epoch 132: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 132: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 373ms/step - loss: 0.4856 - accuracy: 0.7750 - val_loss: 0.4019 - val_accuracy: 0.7992\n",
      "Epoch 133/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.7632\n",
      "Epoch 133: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 133: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4727 - accuracy: 0.7632 - val_loss: 0.4014 - val_accuracy: 0.7912\n",
      "Epoch 134/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.7711\n",
      "Epoch 134: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 134: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4667 - accuracy: 0.7711 - val_loss: 0.4029 - val_accuracy: 0.7952\n",
      "Epoch 135/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.7794\n",
      "Epoch 135: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 135: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4623 - accuracy: 0.7794 - val_loss: 0.4035 - val_accuracy: 0.7992\n",
      "Epoch 136/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4573 - accuracy: 0.7882\n",
      "Epoch 136: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 136: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4573 - accuracy: 0.7882 - val_loss: 0.4002 - val_accuracy: 0.7992\n",
      "Epoch 137/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.7833\n",
      "Epoch 137: val_loss did not improve from 0.40003\n",
      "\n",
      "Epoch 137: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 364ms/step - loss: 0.4659 - accuracy: 0.7833 - val_loss: 0.4006 - val_accuracy: 0.7871\n",
      "Epoch 138/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.7789\n",
      "Epoch 138: val_loss improved from 0.40003 to 0.39897, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 138: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 393ms/step - loss: 0.4559 - accuracy: 0.7789 - val_loss: 0.3990 - val_accuracy: 0.7952\n",
      "Epoch 139/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.7755\n",
      "Epoch 139: val_loss improved from 0.39897 to 0.39821, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 139: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 0.4667 - accuracy: 0.7755 - val_loss: 0.3982 - val_accuracy: 0.7952\n",
      "Epoch 140/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4643 - accuracy: 0.7755\n",
      "Epoch 140: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 140: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4643 - accuracy: 0.7755 - val_loss: 0.3984 - val_accuracy: 0.7992\n",
      "Epoch 141/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.7686\n",
      "Epoch 141: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 141: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4659 - accuracy: 0.7686 - val_loss: 0.3997 - val_accuracy: 0.7952\n",
      "Epoch 142/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.7784\n",
      "Epoch 142: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 142: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.4626 - accuracy: 0.7784 - val_loss: 0.4010 - val_accuracy: 0.7952\n",
      "Epoch 143/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7778\n",
      "Epoch 143: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 143: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4607 - accuracy: 0.7778 - val_loss: 0.4031 - val_accuracy: 0.8032\n",
      "Epoch 144/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.7706\n",
      "Epoch 144: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 144: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4506 - accuracy: 0.7706 - val_loss: 0.4040 - val_accuracy: 0.7992\n",
      "Epoch 145/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4610 - accuracy: 0.7745\n",
      "Epoch 145: val_loss did not improve from 0.39821\n",
      "\n",
      "Epoch 145: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4610 - accuracy: 0.7745 - val_loss: 0.3998 - val_accuracy: 0.7992\n",
      "Epoch 146/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.7745\n",
      "Epoch 146: val_loss improved from 0.39821 to 0.39755, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 146: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 405ms/step - loss: 0.4700 - accuracy: 0.7745 - val_loss: 0.3976 - val_accuracy: 0.7952\n",
      "Epoch 147/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.7892\n",
      "Epoch 147: val_loss improved from 0.39755 to 0.39669, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 147: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 399ms/step - loss: 0.4586 - accuracy: 0.7892 - val_loss: 0.3967 - val_accuracy: 0.7952\n",
      "Epoch 148/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4542 - accuracy: 0.7853\n",
      "Epoch 148: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 148: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.4542 - accuracy: 0.7853 - val_loss: 0.3975 - val_accuracy: 0.7871\n",
      "Epoch 149/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.7794\n",
      "Epoch 149: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 149: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4587 - accuracy: 0.7794 - val_loss: 0.4001 - val_accuracy: 0.7871\n",
      "Epoch 150/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.7799\n",
      "Epoch 150: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 150: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4608 - accuracy: 0.7799 - val_loss: 0.4048 - val_accuracy: 0.7871\n",
      "Epoch 151/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.7779\n",
      "Epoch 151: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 151: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.4598 - accuracy: 0.7779 - val_loss: 0.4066 - val_accuracy: 0.7912\n",
      "Epoch 152/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.7902\n",
      "Epoch 152: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 152: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4415 - accuracy: 0.7902 - val_loss: 0.4056 - val_accuracy: 0.7791\n",
      "Epoch 153/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.7789\n",
      "Epoch 153: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 153: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 0.4532 - accuracy: 0.7789 - val_loss: 0.4051 - val_accuracy: 0.7791\n",
      "Epoch 154/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.7876\n",
      "Epoch 154: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 154: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4470 - accuracy: 0.7876 - val_loss: 0.4023 - val_accuracy: 0.7831\n",
      "Epoch 155/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.7789\n",
      "Epoch 155: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 155: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4527 - accuracy: 0.7789 - val_loss: 0.4032 - val_accuracy: 0.7912\n",
      "Epoch 156/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4468 - accuracy: 0.7868\n",
      "Epoch 156: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 156: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4468 - accuracy: 0.7868 - val_loss: 0.4050 - val_accuracy: 0.7831\n",
      "Epoch 157/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.7868\n",
      "Epoch 157: val_loss did not improve from 0.39669\n",
      "\n",
      "Epoch 157: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 363ms/step - loss: 0.4486 - accuracy: 0.7868 - val_loss: 0.3995 - val_accuracy: 0.7871\n",
      "Epoch 158/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.7843\n",
      "Epoch 158: val_loss improved from 0.39669 to 0.39648, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 158: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 0.4618 - accuracy: 0.7843 - val_loss: 0.3965 - val_accuracy: 0.7871\n",
      "Epoch 159/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.7902\n",
      "Epoch 159: val_loss did not improve from 0.39648\n",
      "\n",
      "Epoch 159: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 379ms/step - loss: 0.4416 - accuracy: 0.7902 - val_loss: 0.3972 - val_accuracy: 0.7912\n",
      "Epoch 160/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4607 - accuracy: 0.7770\n",
      "Epoch 160: val_loss did not improve from 0.39648\n",
      "\n",
      "Epoch 160: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.4607 - accuracy: 0.7770 - val_loss: 0.4024 - val_accuracy: 0.7952\n",
      "Epoch 161/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.7868\n",
      "Epoch 161: val_loss did not improve from 0.39648\n",
      "\n",
      "Epoch 161: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4523 - accuracy: 0.7868 - val_loss: 0.4021 - val_accuracy: 0.7912\n",
      "Epoch 162/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.7971\n",
      "Epoch 162: val_loss did not improve from 0.39648\n",
      "\n",
      "Epoch 162: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4466 - accuracy: 0.7971 - val_loss: 0.3990 - val_accuracy: 0.7831\n",
      "Epoch 163/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.7925\n",
      "Epoch 163: val_loss improved from 0.39648 to 0.39458, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 163: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 403ms/step - loss: 0.4415 - accuracy: 0.7925 - val_loss: 0.3946 - val_accuracy: 0.7952\n",
      "Epoch 164/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.7760\n",
      "Epoch 164: val_loss did not improve from 0.39458\n",
      "\n",
      "Epoch 164: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4541 - accuracy: 0.7760 - val_loss: 0.3984 - val_accuracy: 0.7952\n",
      "Epoch 165/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.7863\n",
      "Epoch 165: val_loss did not improve from 0.39458\n",
      "\n",
      "Epoch 165: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4435 - accuracy: 0.7863 - val_loss: 0.3986 - val_accuracy: 0.7831\n",
      "Epoch 166/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.7750\n",
      "Epoch 166: val_loss did not improve from 0.39458\n",
      "\n",
      "Epoch 166: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 373ms/step - loss: 0.4508 - accuracy: 0.7750 - val_loss: 0.3955 - val_accuracy: 0.7912\n",
      "Epoch 167/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.7873\n",
      "Epoch 167: val_loss did not improve from 0.39458\n",
      "\n",
      "Epoch 167: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 406ms/step - loss: 0.4538 - accuracy: 0.7873 - val_loss: 0.3966 - val_accuracy: 0.7791\n",
      "Epoch 168/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.7833\n",
      "Epoch 168: val_loss improved from 0.39458 to 0.39288, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 168: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 0.4579 - accuracy: 0.7833 - val_loss: 0.3929 - val_accuracy: 0.7831\n",
      "Epoch 169/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.7882\n",
      "Epoch 169: val_loss improved from 0.39288 to 0.39227, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 169: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 0.4535 - accuracy: 0.7882 - val_loss: 0.3923 - val_accuracy: 0.7831\n",
      "Epoch 170/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.7912\n",
      "Epoch 170: val_loss did not improve from 0.39227\n",
      "\n",
      "Epoch 170: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.4454 - accuracy: 0.7912 - val_loss: 0.3933 - val_accuracy: 0.7992\n",
      "Epoch 171/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.7819\n",
      "Epoch 171: val_loss did not improve from 0.39227\n",
      "\n",
      "Epoch 171: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4558 - accuracy: 0.7819 - val_loss: 0.3944 - val_accuracy: 0.7952\n",
      "Epoch 172/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.7686\n",
      "Epoch 172: val_loss did not improve from 0.39227\n",
      "\n",
      "Epoch 172: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4634 - accuracy: 0.7686 - val_loss: 0.3944 - val_accuracy: 0.7912\n",
      "Epoch 173/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.7789\n",
      "Epoch 173: val_loss did not improve from 0.39227\n",
      "\n",
      "Epoch 173: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4690 - accuracy: 0.7789 - val_loss: 0.3932 - val_accuracy: 0.7952\n",
      "Epoch 174/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.7936\n",
      "Epoch 174: val_loss improved from 0.39227 to 0.38829, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 174: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 394ms/step - loss: 0.4379 - accuracy: 0.7936 - val_loss: 0.3883 - val_accuracy: 0.8032\n",
      "Epoch 175/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4546 - accuracy: 0.7686\n",
      "Epoch 175: val_loss improved from 0.38829 to 0.38632, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 175: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 0.4546 - accuracy: 0.7686 - val_loss: 0.3863 - val_accuracy: 0.7952\n",
      "Epoch 176/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4535 - accuracy: 0.7783\n",
      "Epoch 176: val_loss did not improve from 0.38632\n",
      "\n",
      "Epoch 176: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 370ms/step - loss: 0.4535 - accuracy: 0.7783 - val_loss: 0.3888 - val_accuracy: 0.8072\n",
      "Epoch 177/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.7809\n",
      "Epoch 177: val_loss improved from 0.38632 to 0.38552, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 177: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 390ms/step - loss: 0.4475 - accuracy: 0.7809 - val_loss: 0.3855 - val_accuracy: 0.8032\n",
      "Epoch 178/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.7877\n",
      "Epoch 178: val_loss did not improve from 0.38552\n",
      "\n",
      "Epoch 178: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.4457 - accuracy: 0.7877 - val_loss: 0.3887 - val_accuracy: 0.7952\n",
      "Epoch 179/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4416 - accuracy: 0.7931\n",
      "Epoch 179: val_loss did not improve from 0.38552\n",
      "\n",
      "Epoch 179: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.4416 - accuracy: 0.7931 - val_loss: 0.3872 - val_accuracy: 0.8032\n",
      "Epoch 180/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.7799\n",
      "Epoch 180: val_loss improved from 0.38552 to 0.38381, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 180: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 398ms/step - loss: 0.4567 - accuracy: 0.7799 - val_loss: 0.3838 - val_accuracy: 0.8072\n",
      "Epoch 181/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.7794\n",
      "Epoch 181: val_loss improved from 0.38381 to 0.38243, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 181: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 393ms/step - loss: 0.4529 - accuracy: 0.7794 - val_loss: 0.3824 - val_accuracy: 0.8112\n",
      "Epoch 182/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.7990\n",
      "Epoch 182: val_loss improved from 0.38243 to 0.38133, saving model to vgg16_model.h5\n",
      "\n",
      "Epoch 182: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 13s 394ms/step - loss: 0.4380 - accuracy: 0.7990 - val_loss: 0.3813 - val_accuracy: 0.8032\n",
      "Epoch 183/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.7917\n",
      "Epoch 183: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 183: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 374ms/step - loss: 0.4494 - accuracy: 0.7917 - val_loss: 0.3845 - val_accuracy: 0.7992\n",
      "Epoch 184/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.7740\n",
      "Epoch 184: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 184: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 374ms/step - loss: 0.4515 - accuracy: 0.7740 - val_loss: 0.3858 - val_accuracy: 0.7992\n",
      "Epoch 185/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.7784\n",
      "Epoch 185: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 185: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.4624 - accuracy: 0.7784 - val_loss: 0.3935 - val_accuracy: 0.7992\n",
      "Epoch 186/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.7775\n",
      "Epoch 186: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 186: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4723 - accuracy: 0.7775 - val_loss: 0.3891 - val_accuracy: 0.8032\n",
      "Epoch 187/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.8020\n",
      "Epoch 187: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 187: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4357 - accuracy: 0.8020 - val_loss: 0.3895 - val_accuracy: 0.7952\n",
      "Epoch 188/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.7887\n",
      "Epoch 188: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 188: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4419 - accuracy: 0.7887 - val_loss: 0.3841 - val_accuracy: 0.7992\n",
      "Epoch 189/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4436 - accuracy: 0.7868\n",
      "Epoch 189: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 189: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 372ms/step - loss: 0.4436 - accuracy: 0.7868 - val_loss: 0.3841 - val_accuracy: 0.7992\n",
      "Epoch 190/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.7809\n",
      "Epoch 190: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 190: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4587 - accuracy: 0.7809 - val_loss: 0.3841 - val_accuracy: 0.8072\n",
      "Epoch 191/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4379 - accuracy: 0.7887\n",
      "Epoch 191: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 191: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 0.4379 - accuracy: 0.7887 - val_loss: 0.3865 - val_accuracy: 0.7992\n",
      "Epoch 192/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.7853\n",
      "Epoch 192: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 192: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 376ms/step - loss: 0.4484 - accuracy: 0.7853 - val_loss: 0.3875 - val_accuracy: 0.7952\n",
      "Epoch 193/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.7907\n",
      "Epoch 193: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 193: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4437 - accuracy: 0.7907 - val_loss: 0.3865 - val_accuracy: 0.8032\n",
      "Epoch 194/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4510 - accuracy: 0.7853\n",
      "Epoch 194: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 194: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 378ms/step - loss: 0.4510 - accuracy: 0.7853 - val_loss: 0.3878 - val_accuracy: 0.8032\n",
      "Epoch 195/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.7917\n",
      "Epoch 195: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 195: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 0.4506 - accuracy: 0.7917 - val_loss: 0.3859 - val_accuracy: 0.8072\n",
      "Epoch 196/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.7784\n",
      "Epoch 196: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 196: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 365ms/step - loss: 0.4509 - accuracy: 0.7784 - val_loss: 0.3882 - val_accuracy: 0.7992\n",
      "Epoch 197/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4398 - accuracy: 0.7887\n",
      "Epoch 197: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 197: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 0.4398 - accuracy: 0.7887 - val_loss: 0.3898 - val_accuracy: 0.8032\n",
      "Epoch 198/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.7868\n",
      "Epoch 198: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 198: val_accuracy did not improve from 0.81124\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 0.4506 - accuracy: 0.7868 - val_loss: 0.3888 - val_accuracy: 0.8112\n",
      "Epoch 199/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.7868\n",
      "Epoch 199: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 199: val_accuracy improved from 0.81124 to 0.81526, saving model to vgg16_model.h5\n",
      "32/32 [==============================] - 13s 393ms/step - loss: 0.4491 - accuracy: 0.7868 - val_loss: 0.3866 - val_accuracy: 0.8153\n",
      "Epoch 200/200\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.7819\n",
      "Epoch 200: val_loss did not improve from 0.38133\n",
      "\n",
      "Epoch 200: val_accuracy did not improve from 0.81526\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 0.4517 - accuracy: 0.7819 - val_loss: 0.3817 - val_accuracy: 0.8032\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, steps_per_epoch=32, epochs=200,\n",
    "                              validation_data=(X_val, Y_val),validation_steps=50,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zIzL2C3CMALZ"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "R18f6MbeMAMr",
    "outputId": "d9468fd2-eaf6-4475-ab2b-8348d4b25298"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('TraiN & Val Acc VS Epochs')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "TFQ2CRkJMAOP",
    "outputId": "603bf58e-15ce-445d-ea16-4b62927eca4a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('TraiN & Val Loss VS Epochs')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSCGIVyZMAQ3",
    "outputId": "aab7872a-b8bc-4d99-aad3-f8ae53b16d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 294ms/step - loss: 0.3866 - accuracy: 0.8153\n",
      "Test Loss: 0.3865843117237091\n",
      "Test accuracy: 81.52610659599304\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"vgg16_model.h5\")\n",
    "score = model.evaluate(X_val, Y_val ,verbose=1)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1]*100)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "Z9pbfYDAMATL",
    "outputId": "0890f61b-ce32-41c4-bc35-fb02353f4384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 82ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Confusion Matrix')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "Y_predx = np.argmax(Y_pred, axis = -1)\n",
    "Y_valx = np.argmax(Y_val, axis = -1)\n",
    "cf_matrix = confusion_matrix(Y_valx, Y_predx)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot = labels, fmt = '')\n",
    "plt.title(\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xt3Fwrm6MAVj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dSQINIfMAYH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGNj6lT5MAZ6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4-qwQJHMAco"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCLX0m-KMAfI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-gpTkScMAhU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALITd9zNMAjm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_pFSty9MAmS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuSkfqGpMAp3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftvX0gxtMAsM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw_Zw2e5MAtn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCNMmMYCMAwD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssN_HvCyMAyv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHaT6ctsMA1X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiJLHROBMA3_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU4wYoMKMA5_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQOAinpGMA8b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyUTb1qzMA-x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ2xG25oMBBh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2E_HFmMMBDa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP_fuYJqMBGA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcRyzwXfMBIQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYNP1S6EMBLC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UksH-AAdMBNO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fc57OLmMBPq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBDA4boUMBSQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wS7YELxzMBVC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhcMGb8EMBXe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5miqg0tfMBZz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSK3YGqLMBcJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeBrUB6NMBeT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
